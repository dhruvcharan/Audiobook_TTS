import os
import torch
import soundfile as sf
import numpy as np
from typing import List
from kokoro import KPipeline
import warnings

warnings.filterwarnings("ignore", message=".*resized since it had shape.*")
warnings.filterwarnings("ignore", message=".*dropout option adds dropout after all but last recurrent layer.*")
warnings.filterwarnings("ignore", message=".*`torch.nn.utils.weight_norm` is deprecated.*")

def get_optimal_device() -> str:
    """
    Detects the best available PyTorch device. 
    Prioritizes output for M-series Macs (mps) and NVIDIA GPUs (cuda).
    """
    if torch.backends.mps.is_available():
        print("Hardware Acceleration: Using Apple Silicon MPS")
        return "mps"
    elif torch.cuda.is_available():
        print("Hardware Acceleration: Using NVIDIA CUDA")
        return "cuda"
    else:
        print("Hardware Acceleration: Using CPU (This will be slow)")
        return "cpu"

class AudioGenerator:
    def __init__(self, voice: str = 'bm_lewis'):
        """
        Initializes the Kokoro TTS pipeline on the optimal device.
        Requires internet connection on first run to download the voice model weights 
        to the HuggingFace cache.
        """
        self.device = get_optimal_device()
        self.lang_code = 'a' # American English
        self.voice = voice
        
        print(f"Loading TTS Pipeline with voice '{self.voice}' on {self.device}...")
        self.pipeline = KPipeline(lang_code=self.lang_code, device=self.device, repo_id='hexgrad/Kokoro-82M')

    def generate_chapter_audio(self, chunks: List[str], output_path: str) -> None:
        """
        Iterates over text chunks, generates numpy audio arrays, 
        concatenates them, and saves to a WAV file.
        
        Args:
            chunks: List of text chunks generated by text_chunker.py
            output_path: Path where the chapter audio file (e.g., .wav) should be saved
            
        Returns:
            Tuple[float, float]: (audio_duration_seconds, generation_time_seconds)
        """
        if not chunks:
            print("No text chunks provided for audio generation.")
            return 0.0, 0.0

        all_audio = []
        sample_rate = 24000 # Kokoro default sample rate
        
        # 1-second silence array to inject between paragraphs
        silence_array = np.zeros(int(sample_rate * 1.0), dtype=np.float32)
        
        # For tracking generation time
        import time
        start_time = time.time()
        
        from tqdm import tqdm
        
        # Pass the entire list of chunks directly to the pipeline. 
        # Kokoro handles internal array batching automatically when given an Iterable/List.
        generator = self.pipeline(chunks, voice=self.voice, speed=1.0, split_pattern=r'\n+')
        
        # We wrap the generator in tqdm so the user still sees chunk-level progress as they yield
        for i, (_, _, audio_array) in enumerate(tqdm(generator, total=len(chunks), desc="  Generating Audio Batches", leave=False)):
            if audio_array is not None and len(audio_array) > 0:
                all_audio.append(audio_array)
                
            # Inject a 1-second silence if this chunk ended a paragraph, 
            # and it isn't the absolute last chunk in the chapter
            if i < len(chunks) - 1:
                # We can map the generator index back to the chunks array to check formatting
                original_text = chunks[i]
                if original_text.endswith('\n\n') or original_text.endswith('\n'):
                    all_audio.append(silence_array)
        
        generation_time = time.time() - start_time
        
        if not all_audio:
            print("Warning: TTS failed to generate any audio arrays.")
            return 0.0, 0.0
            
        # Concatenate all numpy audio chunks into one large array
        final_audio = np.concatenate(all_audio)
        
        # Ensure output directory exists (if output_path includes one)
        out_dir = os.path.dirname(output_path)
        if out_dir:
            os.makedirs(out_dir, exist_ok=True)
        
        # Write to disk using SoundFile
        sf.write(output_path, final_audio, sample_rate)
        
        # Calculate resulting duration
        audio_duration = len(final_audio) / sample_rate
        return audio_duration, generation_time

if __name__ == "__main__":
    # Simple self-test code block
    generator = AudioGenerator()
    test_chunks = ["Hello world, this is a test of the on-device audio generator running on your powerful M5 Mac."]
    generator.generate_chapter_audio(test_chunks, "./test_output.wav")
    print("Test audio generated at ./test_output.wav - Please listen to it!")
